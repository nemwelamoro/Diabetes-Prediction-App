# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns


# Import and read dataset in csv file
diabetes_data = pd.read_csv("C:/Users/FBI/Downloads/Diabetes_Prediction_2018_2022.csv")


#pre processing
# create heatmap with seaborn for missing values
sns.heatmap(diabetes_data.isnull(), cmap="YlGnBu")

plt.show()

# Check and print missing values
print(diabetes_data.isnull().sum())

# remove column1 with missing values
diabetes_data= diabetes_data.drop('Column1' , axis=1)

#data types stored
diabetes_data.dtypes
#all are int64

#the z-score method is used to identify data points that are more than a certain number of standard deviations away from the mean
# calculate the z-score for each column
z_scores = np.abs((diabetes_data - diabetes_data.mean()) / diabetes_data.std())

# set a threshold z-score
threshold = 3

# identify potential outliers
outliers = diabetes_data[(z_scores > threshold).any(axis=1)]

# show the outliers
print(outliers)

# remove the rows with outliers
diabetes_data = diabetes_data[(z_scores <= threshold).all(axis=1)]

#descriptive statistics our dataset
print(diabetes_data.describe())


# Split the data into independent (features) and dependent (target) variables
X = diabetes_data.drop('Diabetes_012', axis=1)
y = diabetes_data['Diabetes_012']

# Split the dataset at 80% for training and 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Use StandardScaler to scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# Training the model
# Create a Decision Tree Classifier object and fit the model to the training data
dtc = DecisionTreeClassifier(random_state=42)
dtc.fit(X_train, y_train)

# Pre Pruning
# Set the maximum depth of the decision tree to avoid overfitting
dtc_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)
dtc_pruned.fit(X_train, y_train)

# Overfitting
# Create a new Decision Tree Classifier object with more complex settings to observe overfitting
dtc_overfit = DecisionTreeClassifier(max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)
dtc_overfit.fit(X_train, y_train)


# Score the model
# Predict the outcome using the testing data
y_pred = dtc.predict(X_test)

# Calculate the accuracy score of the model
print('Accuracy Score:', accuracy_score(y_test, y_pred))

# Create a confusion matrix to evaluate the performance of the model
conf_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:', conf_matrix)


#derive feature importance
importances = dtc.feature_importances_
columns= X.columns
i= 0

while i < len(columns) :
    print(f" the importance of features '{columns[i]}' is {round(importances[i]* 100,2)}%.")
    i += 1


# Gridsearch for hyperparameters
params = {'max_depth': [3, 5, 7, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5)
grid_search.fit(X_train, y_train)
print('Best Estimator:', grid_search.best_estimator_)

# Convert trained model to a TensorFlow Lite model
converter = tf.lite.TFLiteConverter.from_sklearn(dtc)
tflite_model = converter.convert()
open("diabetes_prediction.tflite", "wb").write(tflite_model)


import pickle
# pickle the model to a file
with open("C:/Users/FBI/Downloads/model.pkl", 'wb') as f:
    pickle.dump(dtc, f)
